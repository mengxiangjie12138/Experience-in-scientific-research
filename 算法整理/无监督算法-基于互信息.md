## ## 互信息算法整理

### 1.Learning deep representations by mutual information estimation and maximization ICLR2019

- 输入的图片x使用CNN网络提取特征得到feature maps
- feature maps再经过另一个CNN网络得到特征向量feature vector作为global feature
- 从feature maps中截取一列特征向量作为local feature
- 将feature vector经过两个平行的全连接层得到两个向量，通过这两个向量构造一系列正态分布的均值和方差，计算KL散度（借鉴VAE算法）作为损失的一部分
- 将global feature自身拼接作为真样本，global feature与其他图片的global feature拼接作为假样本，训练global feature的鉴别器，得到global feature与输入之间的互信息损失
- 将global feature与自身图片的local feature拼接作为真样本，global feature与其他图片的local feature拼接作为假样本，训练local feature的鉴别器，计算local feature与输入之间的互信息损失。
- 提取特征之后的下游任务可可变。

![deep-infomax](http://mengxiangjie12138-images.oss-cn-beijing.aliyuncs.com/Deep-infoMax-method.png)

### 2.Learning Discrete Representations via Information Maximizing Self-Augmented Training

- 输入图片x分别经过仿射变换和VAT后得到增强数据x-affine和x-vat，统一计入输入X
- 传入CNN特征提取器得到概率分布Y
- 计算X与Y之间的互信息，根据公式可得：

![IMSAT](http://mengxiangjie12138-images.oss-cn-beijing.aliyuncs.com/IMSAT-f.png)

![IMSAT](http://mengxiangjie12138-images.oss-cn-beijing.aliyuncs.com/IMSAT-method.png)

### 3.Information Clustering for Unsupervised Image Classification and Segmentation  ICCV2019

- 将输入图片x经过仿射变换得到x'；

- 单数个epoch：将x和​x'传入参数共享的成熟网络（ResNet或VGG11）提取特征，经过全连接层得到属于每个类别的概率分布z​和​z'​；

- 单数个epoch：假设z​和​z'​相互独立，根据2得到的边缘概率分布，计算z和z'​的联合概率分布矩阵​P'​；

- 单数个epoch：使用以下公式打破独立性，得到新的联合概率分布矩阵P​；

<img src="https://mengxiangjie12138-images.oss-cn-beijing.aliyuncs.com/IIC-1.png" alt="IIC1" style="zoom:5%;" />

- 单数个epoch：根据P​计算如下互信息作为目标函数，通过最大化目标函数（最小化互信息的相反数）来优化网络，使用BP算法回传；

<img src="https://mengxiangjie12138-images.oss-cn-beijing.aliyuncs.com/IIC-2.png" alt="IIC2" style="zoom:10%;" />

- 偶数个epoch：使用overclustering方法，强制将图片分为更多类别，类比单数个epoch，得到更大的联合概率分布矩阵，计算损失并回传。

 <img src="http://mengxiangjie12138-images.oss-cn-beijing.aliyuncs.com/IIC-method.png" alt="method" style="zoom:50%;" />



















