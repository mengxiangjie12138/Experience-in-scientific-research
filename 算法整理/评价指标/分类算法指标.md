## 分类算法评价指标

### 基本指标

以二分类为例

TP(True Positive)：若一个实例是正类，并且被预测为正类

FN(False Negative)：若一个实例是正类，但是被预测为负类，即为假负类

FP(False Positive)：若一个实例是负类，但是被预测为正类，即为假正类

TN(True Negative)：若一个实例是负类，并且被预测为负类，即为真负类

多分类时所有错分样本均属于负类，转换成二分类问题后使用该指标。

### 常见指标

#### 准确率（Accuracy）

预测正确的样本数量占总量的百分比，多分类问题时用同样的计算方法。在0-1之间，越接近1代表模型效果越好。

<img src="https://mengxiangjie12138-images.oss-cn-beijing.aliyuncs.com/accuracy.png" alt="acc" style="zoom:10%;" />

缺点：数据样本不均衡时不能评价模型性能优劣。

python实现：

```python
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(labels_true, labels_pred)
'''
参数分析
y_true:真实标签
y_pred:预测标签
normalize(True):如果为False，则返回正确样本的数量，如果为True则返回准确率。
'''
'''
返回值
准确率 normalize=True
正确样本的数量 normalize=False
'''
```

#### 精准率(Precision)

在模型预测为正样本的结果中，真正是正样本所占的百分比。多分类问题中取各类的Precision计算平均值。在0-1之间，越接近1代表模型效果越好。

<img src="http://mengxiangjie12138-images.oss-cn-beijing.aliyuncs.com/img/Precision.png" alt="Precision" style="zoom:10%;" />

缺点：如果阈值较高，那么精准率会高，但是会漏掉很多数据。

python实现：

```python
from sklearn.metrics import precision_score
accuracy = precision_score(labels_true, labels_pred)
'''
参数分析
y_true:真实标签
y_pred:预测标签
labels(None):列表. 当average != binary时被包含的标签集合，如果average是None的话还包含它们的顺序. 在数据中存在的标签可以被排除，比如计算一个忽略多数负类的多类平均值时，数据中没有出现的标签会导致宏平均值（marco average）含有0个组件. 对于多标签的目标，标签是列索引. 默认情况下，y_true和y_pred中的所有标签按照排序后的顺序使用.
pos_label(1):字符串或整型，默认为1. 如果average = binary并且数据是二进制时需要被报告的类. 若果数据是多类的或者多标签的，这将被忽略；设置labels=[pos_label]和average != binary就只会报告设置的特定标签的分数.
average(None):字符串，可选值为[None, ‘binary’ (默认), ‘micro’, ‘macro’, ‘samples’, ‘weighted’]. 多类或 者多标签目标需要这个参数. 如果为None，每个类别的分数将会返回. 否则，它决定了数据的平均值类型.
	‘binary’: 仅报告由pos_label指定的类的结果. 这仅适用于目标（y_{true, pred}）是二进制的情况.
	‘micro’: 通过计算总的真正性、假负性和假正性来全局计算指标.
	‘macro’: 为每个标签计算指标，找到它们未加权的均值. 它不考虑标签数量不平衡的情况.
	‘weighted’: 为每个标签计算指标，并通过各类占比找到它们的加权均值（每个标签的正例数）.它解决了’macro’的标签不平衡问题；它可以产生不在精确率和召回率之间的F-score.
	‘samples’: 为每个实例计算指标，找到它们的均值（只在多标签分类的时候有意义，并且和函数accuracy_score不同）.
sample_weight(None):形状为[样本数量]的数组，可选参数. 样本权重.
'''
'''
返回值
Precision分数。
二分类时为单个分数
多分类时为各个类别的分数列表或者取平均后的单个分数，由参数确定。
'''
```

#### 召回率(Recall)

在实际为正样本中，被预测为正样本所占的百分比。多分类问题中取各类的Recall值计算平均值。在0-1之间，越接近1代表模型效果越好。

<img src="https://mengxiangjie12138-images.oss-cn-beijing.aliyuncs.com/Recall.png" alt="Recall" style="zoom:10%;" />

缺点：如果阈值较低，召回率高，但是预测的会很不准确。

python实现：

```python
from sklearn.metrics import recall_score
recall = recall_score(labels_true, labels_pred)
'''
参数分析同Precision
'''
```

#### F1-score

精确率和召回率的调和平均。在0-1之间，越接近1代表模型效果越好。

<img src="https://mengxiangjie12138-images.oss-cn-beijing.aliyuncs.com/f1-score.png" alt="f1-score" style="zoom:10%;" />

python实现：

```python
from sklearn.metrics import f1_score
f1_score = f1_score(labels_true, labels_pred)
'''
参数分析同Precision
'''
```

#### 注：宏平均与微平均的区别

Macro Average 宏平均是指在计算均值时使每个类别具有相同的权重，最后结果是每个类别的指标的算术平均值。

Micro Average 微平均是指计算多分类指标时赋予所有类别的每个样本相同的权重，将所有样本合在一起计算各个指标。

- 如果每个类别的样本数量差不多，那么宏平均和微平均没有太大差异
- 如果每个类别的样本数量差异很大，那么注重样本量多的类时使用微平均，注重样本量少的类时使用宏平均
- 如果微平均大大低于宏平均，那么检查样本量多的类来确定指标表现差的原因
- 如果宏平均大大低于微平均，那么检查样本量少的类来确定指标表现差的原因

### 混淆矩阵

二分类：
|  实际值\预测值  | Positive | Negative |
|  :---  | :---  |  :---  |
| 正 | TP       | FN |
| 负 | FP | TN |

多分类（以3分类为例）：

| 实际值\预测值 | 第1类 | 第2类 | 第3类 |
|  :---  | :---  |  :---  |  ----  |
| 第1类         | - | - | - |
| 第2类         | - | - | - |
| 第3类         | - | - | - |

python实现：

```python
from sklearn.metrics import confusion_matrix
matrix = confusion_matrix(labels_true, labels_pred)
'''
参数分析
y_true:真实标签
y_pred:预测标签
labels(None):索引矩阵的标签列表。这可用于重新排序或选择标签的子集。如果没有给出，那些至少出现一次y_true或y_pred按排序顺序使用的那些。
sample_weight(None):形状为[样本数量]的数组，可选参数. 样本权重.
'''
'''
返回值
array(shape = [n_classes, n_classes] )
'''
```

### ROC曲线和AUC值

相关概念：

<img src="https://mengxiangjie12138-images.oss-cn-beijing.aliyuncs.com/TPR-FPR.png" alt="TPRFPR" style="zoom:10%;" />

画法：

在二分类问题中，我们最终得到的数据是对每一个样本估计其为正的概率值（Score），我们根据每个样本为正的概率大小从大到小排序，然后按照概率从高到低，一次将“Score”值作为阈值threshold，当测试样本的概率大于或等于这个threshold时，我们认为它为正样本，否则为负样本。每次选取一个不同的threshold，就可以得到一组FPR和TPR,即ROC曲线上的一点。

在多分类问题中，可以对每一类的样本转化成二分类问题，为每一类样本画出各自的ROC曲线。

AUC值：

ROC曲线下的面积。AUC的面积越大，分类效果越好。AUC小于1，但因为良好的分类器一定比随机分类效果好，所以 0.5 <= AUC <= 1

评价标准：

评价分类器的优劣，当分类器赋予各类的概率相差较大时（即概率分布趋于one-hot时，也即分类器能明确判定该样本属于某类）效果更好。

python实现（多分类）：

```python
def softmax(x):
    x_row_max = x.max(axis=-1)
    x_row_max = x_row_max.reshape(list(x.shape)[:-1]+[1])
    x = x - x_row_max
    x_exp = np.exp(x)
    x_exp_row_sum = x_exp.sum(axis=-1).reshape(list(x.shape)[:-1]+[1])
    softmax = x_exp / x_exp_row_sum
    return softmax


def ROC_binary_main(labels_true_path, labels_pred_path, present_path, class_num):
    fpr_list = []
    tpr_list = []
	# 各类ROC曲线绘制
    for num in range(0, class_num):
        y_test = np.load(labels_true_path, allow_pickle=True)
        y_predicted = np.load(labels_pred_path, allow_pickle=True)
        acc, y_test, y_predicted = cluster_acc_true_to_pred(y_test, y_predicted, 10)

        y_score_raw = np.load(present_path, allow_pickle=True)
        y_score_new = []
        y_test_new = []
        for y in y_test:
            if y == num:
                y_test_new.append(1)
            else:
                y_test_new.append(0)
        for i in y_score_raw:
            i_new = softmax(i)
            y_score_new.append(i_new[num])

        fpr, tpr, threshold = roc_curve(y_test_new, y_score_new, pos_label=1)
        fpr_list.append(np.mean(fpr))
        tpr_list.append(np.mean(tpr))
        print(fpr)
        # print(fpr)
        # print(tpr)
        # exit()

        roc_auc = auc(fpr, tpr)

        lw = 2
        plt.figure(figsize=(6*1.2, 6))
        plt.plot(fpr, tpr, color='blue',
                 lw=lw, label='AUC = %0.2f' % roc_auc)  # 假正率为横坐标，真正率为纵坐标做曲线
        plt.plot([0, 1], [0, 1], color='blue', lw=lw, linestyle='--')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('ROC plot')
        plt.legend(loc="lower right")
        plt.savefig('ROC{}.pdf'.format(num))
        # plt.show()
        print('ROC plot{} has finished!'.format(num))
    # Overall ROC曲线
    y_test = np.load(labels_true_path, allow_pickle=True)
    y_predicted = np.load(labels_pred_path, allow_pickle=True)
    y_score_new = []
    y_score_raw = np.load(present_path, allow_pickle=True)
    for i in y_score_raw:
        i_new = softmax(i)
        y_score_new.append(np.max(i_new))
    acc, y_test, y_predicted = cluster_acc_true_to_pred(y_test, y_predicted, 20)
    fpr, tpr, threshold = roc_curve(y_test, y_score_new, pos_label=y_predicted)
    roc_auc = auc(fpr, tpr)
    lw = 2
    plt.figure(figsize=(6 * 1.2, 6))
    plt.plot(fpr, tpr, color='blue',
             lw=lw, label='AUC = %0.2f' % roc_auc)  # 假正率为横坐标，真正率为纵坐标做曲线
    plt.plot([0, 1], [0, 1], color='blue', lw=lw, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC overall plot')
    plt.legend(loc="lower right")
    plt.savefig('ROC_overall.pdf'.format(100))
```

示例：

![ROC](https://mengxiangjie12138-images.oss-cn-beijing.aliyuncs.com/ROC-mode.png)

注：虚线代表随机猜测时的ROC曲线








