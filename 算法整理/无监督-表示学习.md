# 无监督-基于表示学习

注：表示学习的下游任务经常用于分类问题，在聚类问题上不一定合适。

### 1.A Simple Framework for Contrastive Learning of Visual Representation  arxiv2020

- 将原始图片x经过两次随机数据增强得到x-i和x-j，分别经过特征提取器得到features  z-i和z-j。
- 最小化z-i和z-j的余弦距离，拉近两张图片的相似度

<img src="http://mengxiangjie12138-images.oss-cn-beijing.aliyuncs.com/SimCLR-loss1.png" alt="Sim" style="zoom:5%;" />

- 以batch为2为例，根据上述方法分别得到features  z-i，z-j，z-m，z-n和相似度s-ij，s-im，s-in，s-ji，s-jm，s-jn。类比NCE算法，对第一张图片的loss-NCE进行计算

<img src="http://mengxiangjie12138-images.oss-cn-beijing.aliyuncs.com/SimCLR-loss2.png" alt="SimCLR" style="zoom:15%;" />

- 相应的计算第二张图片的loss-NCE，并对一个batch内的Loss取平均。

![SimCLR](http://mengxiangjie12138-images.oss-cn-beijing.aliyuncs.com/SimCLR-method.png)